{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impact analysis of CMA-ES modules on black-box optimization tasks.\n",
    "\n",
    "This Python Notebook records the data analysis that was performed for the paper **Algorithm Configuration Data Mining for CMA Evolution Strategies** by **Sander van Rijn, Hao Wang, Bas van Stein and Thomas Bäck**, accepted for publication at GECCO 2017, Berlin. The dataset consisting of pre-processed .arff files has been generated based on a previous paper **Evolving the Structure of Evolution Strategies** by **Sander van Rijn, Hao Wang, Matthijs van Leeuwen and Thomas Bäck**.\n",
    "\n",
    "\n",
    "## Data Format\n",
    "\n",
    "Filenames follow the format:\n",
    "\n",
    "    bruteforce_<D>_f<F>.arff\n",
    "\n",
    "    | Description          | Values                       |\n",
    "    |----------------------|------------------------------|\n",
    "    | Dimensionality <D>   | 2, 3, 5, 10, 15, ..., 35, 40 |\n",
    "    | BBOB Function ID <F> | 1, 2, 3, 4, 5, ....., 23, 24 |\n",
    "\n",
    "\n",
    "Data entries consist of a configuration specification $c \\in \\{0,1\\}^9 \\times \\{0,1,2\\}^2$ and associated quality measure $q \\in [0, 2]$.\n",
    "Using SciPy's included `arff` reader, the data can easily be read in.\n",
    "\n",
    "\n",
    "    Configuration (1-11)  | q-measure\n",
    "    ----------------------+-----------------\n",
    "    0,0,0,0,0,0,0,0,0,0,0 | 0.00573046875\n",
    "    0,0,0,0,0,0,0,0,0,0,1 | 0.00549609375\n",
    "    0,0,0,0,0,0,0,0,0,0,2 | 0.005490234375\n",
    "    0,0,0,0,0,0,0,0,0,1,0 | 0.0044736328125\n",
    "    [...]                 | [...]\n",
    "    1,1,1,1,1,1,1,1,1,1,2 | 0.0120693359375\n",
    "    1,1,1,1,1,1,1,1,1,2,0 | 0.240296875\n",
    "    1,1,1,1,1,1,1,1,1,2,1 | 0.0112963867188\n",
    "    1,1,1,1,1,1,1,1,1,2,2 | 0.0117387695313\n",
    "\n",
    "## Author\n",
    "\n",
    "Sander van Rijn &lt;svr003@gmail.com; s.j.van.rijn@liacs.leidenuniv.nl&gt; <br>\n",
    "Last update: 2019-11-26"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First cell to be run, containing all imports and default settings to be used throughout the rest of this notebook.\n",
    "\n",
    "Some data-specific information is also pre-set here, such as the input file format, list of dimensionalities, BBOB function ID's and module names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T13:31:58.532697Z",
     "start_time": "2017-05-03T13:31:57.557843Z"
    },
    "code_folding": [],
    "init_cell": false
   },
   "outputs": [],
   "source": [
    "# Imports + definitions\n",
    "%matplotlib inline\n",
    "\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import os\n",
    "import pydot\n",
    "import scipy.io.arff as arff\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from cycler import cycler\n",
    "from IPython.display import Image\n",
    "from itertools import combinations, product\n",
    "from matplotlib import gridspec\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.stats import mannwhitneyu, truncnorm\n",
    "from scipy.stats.stats import pearsonr\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "data_dir = 'arff/'\n",
    "plot_dir = 'plots/'\n",
    "plot_format = 'png'  # Suggested: 'png' for fixed images, 'pdf' for vector graphics\n",
    "\n",
    "np.set_printoptions(precision=4, linewidth=200)\n",
    "plt.rc('axes', prop_cycle=(cycler('color', ['#1f77b4', '#ff7f0e', '#2ca02c', \n",
    "                                            '#d62728', '#9467bd', '#8c564b', \n",
    "                                            '#e377c2', '#7f7f7f', '#bcbd22', \n",
    "                                            '#1f77b4', '#ff7f0e']) +\n",
    "                           cycler('linestyle', ['-', '-', '-', \n",
    "                                                '-', '-', '-', \n",
    "                                                '-', '-', '-', \n",
    "                                                '--', '--'])))\n",
    "\n",
    "# Some matplotlib typesetting parameters to avoid Type 3 fonts\n",
    "plt.rcParams['text.usetex'] = True                                             # Let TeX do the typsetting\n",
    "plt.rcParams['text.latex.preamble'] = ['\\\\usepackage{sansmath}', '\\\\sansmath'] # Force sans-serif math mode (for axes labels)\n",
    "plt.rcParams['font.family'] = 'sans-serif'                                     # ... for regular text\n",
    "plt.rcParams['font.sans-serif'] = 'Helvetica'                                  # Choose a nice font here\n",
    "\n",
    "\n",
    "\n",
    "fname = 'bruteforce_{}_f{}.arff'\n",
    "dims = [2, 3, 5, 10, 15, 20, 25, 30, 35, 40]\n",
    "fids = range(1, 25)\n",
    "modules = ['Active', 'Elitism', 'Mirrored', 'Orthogonal', 'Sequential', 'Threshold', 'TPA', 'Pairwise', 'Weights', 'Base-Sampler', '(B)IPOP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T13:32:57.591233Z",
     "start_time": "2017-05-03T13:32:48.570291Z"
    },
    "code_folding": [],
    "init_cell": false
   },
   "outputs": [],
   "source": [
    "# Load all data from files into 'cases'\n",
    "cases = defaultdict(dict)\n",
    "for dim, fid in product(dims, fids):\n",
    "    case = arff.loadarff(data_dir + fname.format(dim, fid))[0]\n",
    "    case = np.array([list(dat) for dat in case])\n",
    "    cases[dim][fid] = case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "# To select a smaller set of experiments, #\n",
    "#        make your selection here:        #\n",
    "###########################################\n",
    "\n",
    "### Overwrite `dims` and `fids` by editing these two lines:\n",
    "### E.g. the subset `dims=[2,3]; fids=[1,2,3]` should not take too long\n",
    "# dims = [2, 3]            # orig: [2, 3, 5, 10, 15, 20, 25, 30, 35, 40]\n",
    "# fids = [1, 2, 3]         # orig: range(1, 25)\n",
    "\n",
    "### These are now set based on the latest choice\n",
    "experiments = list(product(dims, fids))\n",
    "labels = list(product(dims, fids, modules))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the $q$-measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to give an idea of what data we're dealing with: lets plot the data we have.\n",
    "\n",
    "The cell below creates a graph for each of the 24 functions in the BBOB suite.\n",
    "For each dimensionality, all 4608 $q$-measure values are plotted, sorted by $q$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T13:33:19.458168Z",
     "start_time": "2017-05-03T13:32:57.592304Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "for fid in fids:\n",
    "    plt.figure(figsize=(9, 6))\n",
    "    plt.title(\"Algorithm quality vs. Rank for F{}\".format(fid))\n",
    "    \n",
    "    for dim in dims:\n",
    "        temp = cases[dim][fid][ cases[dim][fid][:,11].argsort() ][:,11]\n",
    "        plt.plot(temp, label=\"{}D\".format(dim))\n",
    "\n",
    "    plt.plot([1]*temp.shape[0], 'k')\n",
    "    plt.legend(loc=0)\n",
    "    plt.ylim([0,2])\n",
    "    plt.xlim([0,4608])\n",
    "    plt.xlabel('Rank')\n",
    "    plt.ylabel('Quality')\n",
    "    plt.tight_layout()\n",
    "            \n",
    "    plt.savefig(plot_dir + \"q-measures_f{}.{}\".format(fid, plot_format))\n",
    "    \n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T13:33:19.471856Z",
     "start_time": "2017-05-03T13:33:19.459571Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "Image(filename=plot_dir + 'q-measures_f10.' + plot_format)  # An example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because configurations consist of 11 choices (i.e. \"should this module be on or off?\"),\n",
    "decision trees are a very natural first step into visualizing what it means to include a module.\n",
    "\n",
    "Each tree is created by giving the module configuration as the decision variables,\n",
    "and the $q$-measure as the prediction value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full size trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T13:42:59.527008Z",
     "start_time": "2017-05-03T13:33:19.473372Z"
    }
   },
   "outputs": [],
   "source": [
    "features = modules\n",
    "\n",
    "for dim, fid in experiments:\n",
    "    clf = tree.DecisionTreeRegressor(min_samples_split=50, min_samples_leaf=20)\n",
    "    clf = clf.fit(cases[dim][fid][:,:11], cases[dim][fid][:,11])\n",
    "\n",
    "    dot_data = tree.export_graphviz(clf, out_file=None,\n",
    "                                    feature_names=features,\n",
    "                                    class_names=['Fitness'],\n",
    "                                    filled=True,\n",
    "                                    rounded=True,\n",
    "                                    )\n",
    "    graph = pydot.graph_from_dot_data(dot_data)[0]\n",
    "    graph.write_png(plot_dir + \"regressor_tree_{}_f{}.{}\".format(dim, fid, plot_format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename=plot_dir + 'regressor_tree_2_f1.' + plot_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can tell, these trees are way too big. There is basically no useful way to read\n",
    "anything from these trees.\n",
    "\n",
    "Let's try smaller trees instead:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T13:44:19.479233Z",
     "start_time": "2017-05-03T13:42:59.528082Z"
    }
   },
   "outputs": [],
   "source": [
    "features = modules\n",
    "\n",
    "for dim, fid in experiments:\n",
    "    clf = tree.DecisionTreeRegressor(max_depth=4, min_samples_split=50, min_samples_leaf=20)\n",
    "    clf = clf.fit(cases[dim][fid][:,:11], cases[dim][fid][:,11])\n",
    "\n",
    "    dot_data = tree.export_graphviz(clf, out_file=None,\n",
    "                                    feature_names=features,\n",
    "                                    class_names=['Fitness'],\n",
    "                                    filled=True,\n",
    "                                    rounded=True,\n",
    "                                    )\n",
    "    graph = pydot.graph_from_dot_data(dot_data)[0]\n",
    "    graph.write_png(plot_dir + \"small_regressor_tree_{}_f{}.{}\".format(dim, fid, plot_format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename=plot_dir + 'small_regressor_tree_2_f1.' + plot_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T13:45:53.337152Z",
     "start_time": "2017-05-03T13:44:19.480423Z"
    }
   },
   "outputs": [],
   "source": [
    "features = modules\n",
    "\n",
    "best_x = 250\n",
    "\n",
    "for dim, fid in experiments:\n",
    "    clf = tree.DecisionTreeRegressor(min_samples_split=20, min_samples_leaf=10)\n",
    "    clf = clf.fit(cases[dim][fid][:best_x,:11], cases[dim][fid][:best_x,11])\n",
    "\n",
    "    dot_data = tree.export_graphviz(clf, out_file=None,\n",
    "                                    feature_names=features,\n",
    "                                    class_names=['Fitness'],\n",
    "                                    filled=True,\n",
    "                                    rounded=True,\n",
    "                                    )\n",
    "    graph = pydot.graph_from_dot_data(dot_data)[0]\n",
    "    graph.write_png(plot_dir + \"best_{}_regressor_tree_{}_f{}.{}\".format(best_x, dim, fid, plot_format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename=plot_dir + 'best_{}_regressor_tree_2_f1.' + plot_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using smaller trees, it is much easier to see what is going on, and which module has any influence.\n",
    "However, there are still just too many of them to quickly get a good idea of what's happening.\n",
    "\n",
    "Besides, how good are these trees anyway? We need some numerical measures\n",
    "to get a better idea of how useful these trees are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T13:46:13.082166Z",
     "start_time": "2017-05-03T13:45:53.338177Z"
    }
   },
   "outputs": [],
   "source": [
    "scores = {dim: np.zeros(len(fids)) for dim in dims}\n",
    "np.set_printoptions(precision=2, linewidth=200)\n",
    "\n",
    "for dim in dims:\n",
    "    for i_fid, fid in enumerate(fids):\n",
    "        clf = tree.DecisionTreeRegressor(min_samples_split=50, min_samples_leaf=20)\n",
    "        clf = clf.fit(cases[dim][fid][:,:11], cases[dim][fid][:,11])\n",
    "        scores[dim][i_fid] = np.mean(cross_val_score(clf, cases[dim][fid][:,:11], cases[dim][fid][:,11], cv=20))\n",
    "        \n",
    "    print(\"{}D: {}\".format(dim, scores[dim]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Higher values mean a tree provides a good prediction/classification of the $q$-measure\n",
    "value based on the configuration. It's clear that some trees are good, but others not so much.\n",
    "\n",
    "This still doesn't give any information about what each _module_ is doing to the performance.\n",
    "To get an idea of this, we can calculate so-called _feature importance_ for each module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-08T16:47:39.980107",
     "start_time": "2017-02-08T16:47:39.978320"
    }
   },
   "source": [
    "## Feature Importance based on Regressor Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First create a large number of random decision trees (e.g. 250).\n",
    "Feature_importance scores can be easily retrieved as `forest.feature_importances_`\n",
    "\n",
    "These scores are calculated based on how often and high up the tree each feature is used\n",
    "for splitting a node. Based on an example from the official [documentation](http://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T13:50:35.899798Z",
     "start_time": "2017-05-03T13:46:13.083103Z"
    }
   },
   "outputs": [],
   "source": [
    "all_importances = np.zeros((len(dims), len(fids), len(modules)))\n",
    "\n",
    "for i_dim, dim in enumerate(dims):\n",
    "    for i_fid, fid in enumerate(fids):\n",
    "        forest = ExtraTreesRegressor(n_estimators=250, min_samples_leaf=20)\n",
    "        forest = forest.fit(cases[dim][fid][:,:11], cases[dim][fid][:,11])\n",
    "        all_importances[i_dim,i_fid,:] = forest.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T13:50:35.905487Z",
     "start_time": "2017-05-03T13:50:35.900789Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_importances = np.mean(np.mean(all_importances, axis=0), axis=0)\n",
    "feature_indices = np.argsort(feature_importances)[::-1]\n",
    "\n",
    "# Print the feature importances\n",
    "print(\"Feature importances:\")\n",
    "for f in range(len(feature_indices)):\n",
    "    print(\"{0: <12} | {1}\".format(modules[f], feature_importances[f]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relative Activation Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an idea of how frequent a certain module is chosen, the cumulative sum versus the ranking can be plotted. This works because a configuration consists purely of 0's, 1's and occaisionally 2's. As all possible configurations have been tested, the total sum will always end up at the same fixed value. For the first 9 modules with options 0 and 1 each, the total will always be 2304\\*0 + 2304\\*1 = 2304, while for the last two modules with options 0, 1 and 2, the total will be 1536\\*0 + 1536\\*1 + 1536\\*2 = 4608.\n",
    "\n",
    "Such a plot already shows if some modules are chosen more often than others, but as they are all constantly increasing and very bunched together, it is hard to see the interesting details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T13:50:37.635208Z",
     "start_time": "2017-05-03T13:50:35.906592Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Starting with the 'default' cumsum plot...\n",
    "fid = 1\n",
    "dim = 2\n",
    "module = modules.index('TPA')\n",
    "\n",
    "# Make sure this specific case is loaded if it wasn't already\n",
    "if cases.get(dim, {}).get(fid, None) is None:\n",
    "    case = arff.loadarff(data_dir + fname.format(dim, fid))[0]\n",
    "    case = np.array([list(dat) for dat in case])\n",
    "    cases[dim][fid] = case\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "lines = plt.plot(np.cumsum(cases[dim][fid][cases[dim][fid][:,11].argsort()][:,:11], axis=0))\n",
    "plt.title(\"Cumulative sum plot of all modules for 2D F1\")\n",
    "plt.ylabel(\"Cumulative sum\")\n",
    "plt.xlabel(\"Configuration\")\n",
    "plt.legend(lines, modules)\n",
    "plt.savefig(plot_dir+'cumsumplot_f1_2d.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So instead we can normalize the values. We start by making all values 0 or 1, so we just count activations. After all, there is no reason for option 2 to be twice as important compared to option 1 for the relevant modules.\n",
    "\n",
    "Next, we are interested in how a module performs compared to the minimum or maximum possible value at that time. As an example: for a module which has been activated in 100 cases, it matters a lot if that has happened in the first 100 or first 1000 configurations. So, we determine the maximum and minimum possible values for any given rank and use those to calculate the distance between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T13:50:40.168062Z",
     "start_time": "2017-05-03T13:50:37.636311Z"
    },
    "code_folding": [],
    "init_cell": false
   },
   "outputs": [],
   "source": [
    "# Create normalization vectors for module progression plots\n",
    "\n",
    "min_column = [0] * 2304\n",
    "min_column.extend(range(1,2305))\n",
    "min_column = np.array(min_column).reshape((4608,1))\n",
    "\n",
    "max_column = range(1, 2305)\n",
    "max_column.extend([2304] * 2304)\n",
    "max_column = np.array(max_column).reshape((4608,1))\n",
    "\n",
    "min_columns = np.repeat(min_column, 9, axis=1)\n",
    "max_columns = np.repeat(max_column, 9, axis=1)\n",
    "\n",
    "\n",
    "min_column = [0] * 1536\n",
    "min_column.extend(range(1, 3073))\n",
    "min_column = np.array(min_column).reshape((4608,1))\n",
    "\n",
    "max_column = range(1, 3073)\n",
    "max_column.extend([3072] * 1536)\n",
    "max_column = np.array(max_column).reshape((4608,1))\n",
    "\n",
    "min_columns = np.hstack((min_columns, min_column, min_column))\n",
    "max_columns = np.hstack((max_columns, max_column, max_column))\n",
    "\n",
    "dists = max_columns - min_columns\n",
    "dists[-1,:] = 1  # Prevent division by 0\n",
    "\n",
    "# Test/comparison for confirmation!\n",
    "temp = cases[2][1][:,:11].copy()\n",
    "\n",
    "# replace all values '2' with '1'\n",
    "two_indices = temp == 2\n",
    "temp[two_indices] = 1\n",
    "\n",
    "fig = plt.figure(figsize=(16, 6))\n",
    "ax1 = fig.add_axes([0.05, 0.1, 0.425, 0.8])\n",
    "ax1.set_title(\"Cumulative sum plot per index (unsorted)\")\n",
    "ax1.set_ylabel(\"Cumulative sum\")\n",
    "ax1.set_xlabel(\"Configuration\")\n",
    "lines = ax1.plot(np.cumsum(temp, axis=0))\n",
    "ax1.legend(lines, ['Index {}'.format(i) for i in range(11)])\n",
    "\n",
    "ax2 = fig.add_axes([0.55, 0.1, 0.425, 0.8])\n",
    "ax2.set_xlabel(\"Configuration\")\n",
    "ax2.set_title(\"Maximum/minimum possible values\")\n",
    "lines = ax2.plot(min_columns[:,8:10], '-.')\n",
    "lines2 = ax2.plot(max_columns[:,8:10], '-.')\n",
    "lines.extend(lines2)\n",
    "ax2.legend(lines, ['Min score (2)', 'Min score (3)', 'Max score (2)', 'Max score (3)'])\n",
    "plt.savefig(plot_dir + \"unsorted_and_minmax_cumsum.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these distances in place, we can normalize each value by subtracting the minimum and dividing by the distance. This way, we see for each module at each rank, how often it is active in the top-$n$ ranks as a fraction of how often it could have been active.\n",
    "\n",
    "In short, this means that a line at the top of the plot will have been active in all the best-ranked configurations, while one at the bottom will have been active in none of them. When it is simply random, the line will hover around 0,5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T13:50:40.190115Z",
     "start_time": "2017-05-03T13:50:40.169234Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the plot as a function to be called again easily\n",
    "\n",
    "def create_relative_activation_plot(dim, fid):\n",
    "    temp = cases[dim][fid][ cases[dim][fid][:,11].argsort() ][:,:11].copy()\n",
    "\n",
    "    # replace all values '2' with '1'\n",
    "    two_indices = temp == 2\n",
    "    temp[two_indices] = 1\n",
    "\n",
    "    temp = np.cumsum(temp, axis=0)\n",
    "    temp = temp - min_columns\n",
    "    temp = temp / dists\n",
    "\n",
    "    cutoff = 100\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.suptitle(\"Progression for {}D F{}\".format(dim, fid))\n",
    "    gs = gridspec.GridSpec(1, 2, width_ratios=[1, 3]) \n",
    "\n",
    "    ax1 = plt.subplot(gs[0])\n",
    "    ax1.plot(temp[:cutoff+1,:])\n",
    "    ax1.set_xlim([0, cutoff])\n",
    "    ax1.set_ylabel('Relative activation frequency')\n",
    "\n",
    "    ax2 = plt.subplot(gs[1], sharey=ax1)\n",
    "    lines = ax2.plot(temp[cutoff//2:,:])\n",
    "    ax2.set_xlim([cutoff//2,4608])\n",
    "    ax2.set_xlabel('Rank')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.subplots_adjust(wspace=0, hspace=0, top=0.88)\n",
    "\n",
    "    # Shrink current axis by 15%\n",
    "    box = ax2.get_position()\n",
    "    ax2.set_position([box.x0, box.y0, box.width * 0.75, box.height])\n",
    "\n",
    "    # Put a legend to the right of the current axis\n",
    "    ax2.legend(loc='center left', bbox_to_anchor=(1, 0.5), handles=lines, labels=modules)\n",
    "\n",
    "    plt.savefig(plot_dir + \"module_progression_{}_f{}.{}\".format(dim, fid, plot_format))\n",
    "\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T13:54:18.527406Z",
     "start_time": "2017-05-03T13:50:40.191135Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create and save plots for all experiments\n",
    "\n",
    "for dim, fid in experiments:\n",
    "    create_relative_activation_plot(dim, fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T13:54:18.538191Z",
     "start_time": "2017-05-03T13:54:18.529563Z"
    }
   },
   "outputs": [],
   "source": [
    "Image(filename=plot_dir + 'module_progression_2_f1.' + plot_format)  # An example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we consider the *Relative Activation Frequency* plots from the previous section to be the 'behavior' of a module in a particular experiment, then it is interesting to compare behaviors between modules and experiments.\n",
    "\n",
    "First, let's just calculate all Pearson R correlations between all behaviors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T13:54:18.675346Z",
     "start_time": "2017-05-03T13:54:18.539804Z"
    },
    "code_folding": [],
    "init_cell": false
   },
   "outputs": [],
   "source": [
    "# Calculate all correlations into 'correlations[dims, fids, modules, dims, fids, modules]'\n",
    "\n",
    "corr_filename = 'correlations.npz'\n",
    "\n",
    "if corr_filename in os.listdir('.'):\n",
    "    print(\"Loading correlations from file '{}'\".format(corr_filename))\n",
    "    corr_file = np.load(corr_filename)\n",
    "    correlations = corr_file['correlations']\n",
    "    corr_dims = corr_file['dims']\n",
    "    corr_fids = corr_file['fids']\n",
    "    corr_mods = corr_file['modules']\n",
    "\n",
    "else:\n",
    "    d = len(dims)\n",
    "    f = len(fids)\n",
    "    g = 11  # Lengh of a genotype\n",
    "\n",
    "    correlations = np.zeros((d, f, g, d, f, g))\n",
    "    data = {x: {} for x in range(d)}\n",
    "    labels = list(product(dims, fids, modules))\n",
    "\n",
    "    for a_dim, dim in enumerate(dims):\n",
    "        for a_fid, fid in enumerate(fids):\n",
    "            temp = cases[dim][fid][ cases[dim][fid][:,11].argsort() ][:,:11].copy()\n",
    "\n",
    "            # replace all values '2' with '1'\n",
    "            temp[temp == 2] = 1\n",
    "\n",
    "            temp = np.cumsum(temp, axis=0)\n",
    "            temp = temp - min_columns\n",
    "            temp = temp / dists\n",
    "\n",
    "            data[a_dim][a_fid] = temp\n",
    "\n",
    "    ndims, nfids, nmods = len(dims), len(fids), len(modules)\n",
    "    combinations = list(product(range(ndims), range(nfids), range(nmods)))\n",
    "    \n",
    "    for (a_dim, a_fid, a_mod), (b_dim, b_fid, b_mod) in product(combinations, repeat=2):\n",
    "        correlations[a_dim, a_fid, a_mod, b_dim, b_fid, b_mod] = pearsonr(data[a_dim][a_fid][:, a_mod], data[b_dim][b_fid][:, b_mod])[0]\n",
    "\n",
    "    np.savez_compressed(\n",
    "        corr_filename,\n",
    "        correlations=correlations,\n",
    "        dims=dims,\n",
    "        fids=fids,\n",
    "        modules=modules,\n",
    "    )\n",
    "    print(\"Correlations stored in file '{}'\".format(corr_filename))\n",
    "    corr_dims = dims\n",
    "    corr_fids = fids\n",
    "    corr_mods = modules\n",
    "\n",
    "corr_labels = list(product(corr_dims, corr_fids, corr_mods))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at this data. In total we have (10\\*24\\*11)^2 correlation values (actually half of that because of symmetry). If these correlation values are just randomly distributed, a histogram should easily show this. Two Gaussian distributions are plotted for comparison: a regular and truncated variant. For more information on the truncated Gaussian, please refer to the relevant SciPy [documentation](https://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.stats.truncnorm.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T13:54:20.093731Z",
     "start_time": "2017-05-03T13:54:18.676683Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Visualize the distribution of correlation values as a histogram (+ expected gaussian to see if they are actually interesting)\n",
    "\n",
    "x = correlations.flatten()\n",
    "mu = np.mean(x)\n",
    "std = np.std(x)\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.title('Histogram of Correlation values (each bar = 0.01)')\n",
    "n, bins, patches = plt.hist(x, 200, normed=1, facecolor='green', alpha=0.75, label='Correlation')\n",
    "y = mlab.normpdf(bins, mu, std)\n",
    "l = plt.plot(bins, y, 'r--', linewidth=1, label='Gaussian')\n",
    "\n",
    "a, b = (-1 - mu) / std, (1 - mu) / std\n",
    "tn = truncnorm(a, b)\n",
    "y = tn.pdf(bins)\n",
    "l = plt.plot(bins, y, 'm--', linewidth=1, label='Truncated Gaussian (-2.13, 2.01)')\n",
    "plt.xlabel('Correlation')\n",
    "plt.ylabel('Prevalence')\n",
    "plt.legend(loc=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that there is indeed a peak of highly correlated behaviors that is not explained by the Gaussian distributions. We can investigate further by plotting the correlations as a heatmap, albeit a rather large one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T13:56:20.511991Z",
     "start_time": "2017-05-03T13:54:20.095100Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Create one giant heatmap (1320*1320)\n",
    "\n",
    "num_combinations = np.product(correlations.shape[:3])\n",
    "temp_corr = correlations.reshape((num_combinations, num_combinations))\n",
    "\n",
    "plt.figure(figsize=(16,12))\n",
    "plt.imshow(temp_corr, cmap='viridis', vmin=-1, vmax=1, origin='lower')\n",
    "plt.title(\"Inter-module behavior correlation\")\n",
    "plt.colorbar(label='Correlation')\n",
    "plt.xlim([0, num_combinations])\n",
    "plt.ylim([0, num_combinations])\n",
    "plt.tight_layout()\n",
    "plt.savefig(plot_dir + \"correlation_heatmap.{}\".format(plot_format))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T13:56:20.278748Z",
     "start_time": "2017-05-03T13:56:20.184042Z"
    }
   },
   "outputs": [],
   "source": [
    "Image(filename=plot_dir + \"correlation_heatmap.\" + plot_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is too big to see anything in besides maybe a vague similarity related to dimensionality.\n",
    "\n",
    "Besides, it is hard to argue what most of these values actually mean: what is the information in the correlation between different modules for different experiments?\n",
    "More useful information is probably shown if these correlations are separately plotted per module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T13:56:37.863003Z",
     "start_time": "2017-05-03T13:56:20.613593Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create heatmaps per module (120*120)\n",
    "\n",
    "num_per_module = len(corr_dims) * len(corr_fids)\n",
    "\n",
    "x = np.arange(num_per_module + 1)\n",
    "y = np.arange(num_per_module + 1)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "temp_corr = correlations.reshape((num_per_module, len(corr_mods), num_per_module, len(corr_mods)))\n",
    "\n",
    "\n",
    "tick_locations = [0.5]\n",
    "tick_locations.extend([i*24 - 0.5 for i in range(1, len(corr_dims)+1)])\n",
    "tick_labels = ['2D F1']\n",
    "tick_labels.extend(['{}D F24'.format(dim) for dim in corr_dims])\n",
    "\n",
    "for mod in range(11):\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.imshow(temp_corr[:,mod,:,mod], cmap='viridis', vmin=-1, vmax=1, origin='lower')\n",
    "    plt.title(\"Behavior correlation for module '{}'\".format(modules[mod]))\n",
    "    plt.xlabel('Experiment')\n",
    "    plt.ylabel('Experiment')\n",
    "    plt.xticks(tick_locations, tick_labels, rotation='vertical')\n",
    "    plt.yticks(tick_locations, tick_labels)\n",
    "    plt.colorbar(label='Correlation')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(plot_dir + \"correlation_heatmap_{}.{}\".format(modules[mod], plot_format))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T13:56:37.876319Z",
     "start_time": "2017-05-03T13:56:37.864736Z"
    }
   },
   "outputs": [],
   "source": [
    "Image(filename=plot_dir + \"correlation_heatmap_Elitism.\" + plot_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A 120\\*120 heatmap is still fairly large, but at least all values are now meaningful. The vague dimensionality correlation we saw earlier is also much more present this time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering by Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous heatmaps suggest that there are some (small) groups of behaviors that correlate highly with each other. Let's find out what clusters of cases we find if we create and plot clusters of highly correlating behaviors.\n",
    "\n",
    "To create the clusters, we use the [NetworkX](https://networkx.github.io/) package to create a network of all behaviors as nodes, where we only add edges between them if their correlation is high enough. You can see below that the code for this is very simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T13:56:37.888686Z",
     "start_time": "2017-05-03T13:56:37.877414Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Cluster creation function definition. Creates clusters by making a graph out of all (dim, fid, mod) pairs that are correlated\n",
    "\n",
    "def cluster_correlation_as_components(corr, corr_filter=0.99, min_size=3):\n",
    "    num_combinations = np.product(corr.shape[:3])\n",
    "    temp = np.triu(corr.reshape((num_combinations, num_combinations)), k=1)\n",
    "\n",
    "    high_corr = temp > corr_filter\n",
    "    high_corr_ind = np.argwhere(high_corr)\n",
    "\n",
    "    G = nx.Graph()\n",
    "    G.add_edges_from(high_corr_ind)\n",
    "    print(\"Number of edges:                \", len(high_corr_ind))\n",
    "    print(\"Number of connected components: \", nx.number_connected_components(G))\n",
    "    print([len(c) for c in sorted(nx.connected_components(G), key=len, reverse=True)])\n",
    "\n",
    "    clusters = [c for c in sorted(nx.connected_components(G), key=len, reverse=True) if len(c) >= min_size]\n",
    "    print(len(clusters))\n",
    "    \n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T13:56:37.939544Z",
     "start_time": "2017-05-03T13:56:37.889834Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Cluster plotting function definition\n",
    "\n",
    "def create_and_save_progression_clusters(clusters, corr_vals, cluster_type='', ignore_triples=False, legend_limit=10):\n",
    "    num_combinations = np.product(corr_vals.shape[:3])\n",
    "    if len(cluster_type) > 0:\n",
    "        \n",
    "        if cluster_type == 'naive':\n",
    "            title_name = 'naive'\n",
    "        elif cluster_type == 'mod-coop':\n",
    "            title_name = 'module cooperation'\n",
    "        elif cluster_type == 'exp-sim':\n",
    "            title_name = 'experiment similarity'\n",
    "        else:\n",
    "            title_name = cluster_type\n",
    "        \n",
    "        cluster_type = cluster_type + '_'\n",
    "\n",
    "    cutoff = 100\n",
    "    for clust_num, cluster in enumerate(clusters):\n",
    "\n",
    "        cluster_corr_indices = np.array(list(combinations([case for case in cluster], 2)))\n",
    "        cluster_corr_vals = [corr_vals.reshape((num_combinations, num_combinations))[a, b] for a, b in cluster_corr_indices]\n",
    "        min_corr_val = np.min(cluster_corr_vals)\n",
    "        \n",
    "        gs = gridspec.GridSpec(1, 2, width_ratios=[1, 3])\n",
    "        plt.figure(figsize=(7.5, 3))\n",
    "        plt.suptitle(\"Progression for {0} cluster (c $>$ {1:.3f})\".format(title_name, min_corr_val))\n",
    "\n",
    "        clust_labels = []\n",
    "        clust_lines = []\n",
    "        for case in cluster:\n",
    "            dim, fid, mod = corr_labels[case]\n",
    "            if ignore_triples is True and mod in ['Base-Sampler', '(B)IPOP']:\n",
    "                continue\n",
    "\n",
    "            mod = modules.index(mod)\n",
    "            temp = cases[dim][fid][ cases[dim][fid][:,11].argsort() ][:,mod].copy()\n",
    "            temp[temp == 2] = 1  # replace all values '2' with '1'\n",
    "\n",
    "            temp = np.cumsum(temp, axis=0)\n",
    "            temp = temp - min_columns[:,mod]\n",
    "            temp = temp / dists[:,mod]\n",
    "\n",
    "            ax1 = plt.subplot(gs[0])\n",
    "            ax1.plot(temp[:cutoff+1])\n",
    "            ax2 = plt.subplot(gs[1], sharey=ax1)\n",
    "            \n",
    "            clust_lines.extend(ax2.plot(temp[cutoff//2:]))\n",
    "            clust_labels.append(\"{} ({}D F{})\".format(modules[mod], dim, fid))\n",
    "\n",
    "        ax1.set_xlim([0, cutoff])\n",
    "        ax1.set_ylim([0,1])\n",
    "        ax1.set_ylabel('Relative activation frequency')\n",
    "        ax2.set_xlim([cutoff//2,4608])\n",
    "        ax2.set_xlabel('Rank')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(wspace=0, hspace=0, top=0.88)\n",
    "\n",
    "        if len(cluster) < legend_limit:\n",
    "            ax2.legend(loc=0, handles=clust_lines, labels=clust_labels)\n",
    "\n",
    "        plt.savefig(plot_dir + \"module_progression_{}cluster_{}_{}.{}\".format(cluster_type, clust_num, len(clusters), plot_format))\n",
    "\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two kinds of clusters that can be considered useful:\n",
    "\n",
    " - Module Cooperations\n",
    " - Experiment similarities\n",
    "\n",
    "For module cooperation, we keep the experiment the same, and see if there are modules that behave similarly. If two modules are activated in (roughly) the same order, their cooperation  probably has a positive influence on the optimization process.\n",
    "\n",
    "Alternatively, we can look for experiment similarity by clustering behavior of the same module over multiple experiments. If a module behaves similarly for multiple experiments, their fitness landscapes are probably related."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module Cooperation clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T13:56:38.220413Z",
     "start_time": "2017-05-03T13:56:37.940788Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Create clusters on 'Module Cooperation'\n",
    "print(\"Module Cooperation (experiment fixed)\")\n",
    "\n",
    "# Create a copy of all correlation values artificially reduced by 1\n",
    "mod_coop_corr = correlations.copy() - 1\n",
    "\n",
    "# Restore the correct correlation values of all cases we are actually interested in\n",
    "for dim in range(len(corr_dims)):\n",
    "    for fid in range(len(corr_fids)):\n",
    "        mod_coop_corr[dim, fid, :, dim, fid, :] = mod_coop_corr[dim, fid, :, dim, fid, :] + 1\n",
    "\n",
    "mod_coop_filter_val = 0.9\n",
    "mod_coop_clusters = cluster_correlation_as_components(mod_coop_corr, corr_filter=mod_coop_filter_val, min_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T14:11:11.210535Z",
     "start_time": "2017-05-03T13:56:38.221405Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Create clustered progression plots\n",
    "create_and_save_progression_clusters(mod_coop_clusters, mod_coop_corr, 'mod-coop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Easiest is to view these resulting plots from the file explorer in the `` plot_dir `` folder*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Similarity Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T14:11:11.291195Z",
     "start_time": "2017-05-03T14:11:11.211744Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Create clusters on 'Experiment Similarity'\n",
    "print(\"Experiment similarity (module fixed)\")\n",
    "\n",
    "# Create a copy of all correlation values artificially reduced by 1\n",
    "exp_sim_corr = correlations.copy() - 1\n",
    "\n",
    "# Restore the correct correlation values of all cases we are actually interested in\n",
    "for mod in range(len(corr_mods)):\n",
    "    exp_sim_corr[:, :, mod, :, :, mod] = exp_sim_corr[:, :, mod, :, :, mod] + 1\n",
    "\n",
    "exp_sim_filter_val = 0.9925\n",
    "exp_sim_clusters = cluster_correlation_as_components(exp_sim_corr, corr_filter=exp_sim_filter_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T14:15:37.059883Z",
     "start_time": "2017-05-03T14:11:11.292275Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Create clustered progression plots\n",
    "create_and_save_progression_clusters(exp_sim_clusters, exp_sim_corr, 'exp-sim')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Easiest is to view these resulting plots from the file explorer in the `` plot_dir `` folder*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impact scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use the $q$-measure from earlier to calculate a numerical *impact* of a module. To do this, we calculate the average $q$-value for all configurations with a module *on* and *off*. By subtracting one from the other, we get a numerical value that indicates how big the impact of a module being active is.\n",
    "\n",
    "To illustrate what this means, let's plot the $q$-values for an example experiment, controlled for whether a single module is active or not. This is done below for the TPA module in the 2-dimensional F1 (Sphere) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T14:15:37.834330Z",
     "start_time": "2017-05-03T14:15:37.060999Z"
    }
   },
   "outputs": [],
   "source": [
    "fid = 1\n",
    "dim = 2\n",
    "module = modules.index('TPA')\n",
    "\n",
    "# Make sure this specific case is loaded if it wasn't already\n",
    "if cases.get(dim, {}).get(fid, None) is None:\n",
    "    case = arff.loadarff(data_dir + fname.format(dim, fid))[0]\n",
    "    case = np.array([list(dat) for dat in case])\n",
    "    cases[dim][fid] = case\n",
    "\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.title(\"Algorithm quality vs. Rank for 2D F{}\".format(fid))\n",
    "\n",
    "\n",
    "temp = cases[dim][fid][ cases[dim][fid][:,11].argsort() ]\n",
    "idx_on = temp[:,module] == 1\n",
    "idx_off = temp[:,module] == 0\n",
    "\n",
    "plt.plot(temp[idx_off][:,11], label='{} off'.format(modules[module]))\n",
    "plt.plot(temp[idx_on][:,11], label='{} on'.format(modules[module]))\n",
    "\n",
    "plt.plot([1]*temp.shape[0], 'k')\n",
    "plt.plot([np.mean(temp[idx_off][:,11])]*2304, color='#1f77b4', linestyle='-.', label='{} off (mean)'.format(modules[module]))\n",
    "plt.plot([np.mean(temp[idx_on][:,11])]*2304, color='#ff7f0e', linestyle='-.', label='{} on (mean)'.format(modules[module]))\n",
    "\n",
    "plt.legend(loc=0)\n",
    "plt.ylim([0,2])\n",
    "plt.xlim([0,2304])\n",
    "plt.xlabel('Rank')\n",
    "plt.ylabel('Quality')\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig(plot_dir + \"visual_impact_score.{}\".format(plot_format))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between the two dashed lines is the impact that this module has. As the mean value for 'TPA on' is below the line for 'TPA off', we can see that activating this module has a *positive* impact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This numerical value is not the only thing of importance though. After all, this impact score is calculated as a difference between two sets, so it is only natural to wonder if this difference is statistically significant. As can already be somewhat seen in the $q$-measure plots, we cannot assume that these $q$-values are distributed normally, so for the statistical test we use the two-tailed [Mann-Whitney U test](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mannwhitneyu.html).\n",
    "\n",
    "First we'll have a look at the impact of *individual* modules, averaged over all 120 experiments. This means that we actually want to create two large sets, one set *on* and another one *off* for each module. For the impact score it would not matter, but averaging the statistical significances of each experiment would make no sense. This cell will also calculate the significance and impact per experiment/module combination and store them, just because we're already creating the subsets for each separate case anyway. These values are used further on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T14:15:43.697299Z",
     "start_time": "2017-05-03T14:15:37.835277Z"
    }
   },
   "outputs": [],
   "source": [
    "single_impact = np.zeros((len(dims), len(fids), len(modules)))\n",
    "single_significance = np.zeros((len(dims), len(fids), len(modules)))\n",
    "impact_calc_data = {dim: {fid: {} for fid in fids} for dim in dims}\n",
    "\n",
    "for i_dim, dim in enumerate(dims):\n",
    "    for i_fid, fid in enumerate(fids):\n",
    "        temp = cases[dim][fid]\n",
    "        \n",
    "        for mod in range(len(modules)):\n",
    "            indices_on = temp[:,mod] > 0\n",
    "            indices_off = temp[:,mod] < 1\n",
    "\n",
    "            impact_calc_data[dim][fid][mod] = (temp[indices_on,11], temp[indices_off,11])\n",
    "            \n",
    "            mean_on = np.mean(temp[indices_on,11])\n",
    "            mean_off = np.mean(temp[indices_off,11])\n",
    "            \n",
    "            single_impact[i_dim, i_fid, mod] = mean_off - mean_on\n",
    "            single_significance[i_dim, i_fid, mod] = mannwhitneyu(temp[indices_on,11],\n",
    "                                                                  temp[indices_off,11],\n",
    "                                                                  alternative='two-sided')[1]\n",
    "\n",
    "print(\"Module name  | Impact             p-value\")\n",
    "print(\"-------------|--------------------------------------\")\n",
    "for mod in range(len(modules)):\n",
    "    data_on, data_off = zip(*[impact_calc_data[dim][fid][mod] for dim in dims for fid in fids])\n",
    "    data_on = np.array(data_on).reshape((-1,1))\n",
    "    data_off = np.array(data_off).reshape((-1,1))\n",
    "    \n",
    "    total_impact = np.mean(data_off) - np.mean(data_on)\n",
    "    total_p_value = mannwhitneyu(data_on, data_off, alternative='two-sided')[1]\n",
    "    \n",
    "    print(\"{0: <12} | {1: <16}   {2: <20}\".format(modules[mod],\n",
    "          total_impact, total_p_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the clearest generic information we now have on these separate modules: what their impact is compared to a [0,2] scale, and the statistical significance of that value. Especially the very low p-values are a great result! The lower impact values such as 0.01 for *Mirrored* should of course be taken with a grain of salt, but impact scores for *Elitism* and *Threshold* really stand out.\n",
    "\n",
    "Next step is to separate the information out into the 120 different experiments, as we can only learn so much from the aggregate. Let's create a scatter-plot of the impact vs. significance for each of the 120\\*11 = 1320 experiment/module combinations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T14:16:09.343337Z",
     "start_time": "2017-05-03T14:15:43.699014Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,6))\n",
    "fig.suptitle('Impact score and corresponding significance test values')\n",
    "\n",
    "ax1 = fig.add_axes([0.05, 0.05, 0.55, 0.85])\n",
    "ax1.set_title('Impact vs p-value')\n",
    "ax1.scatter(single_impact.reshape((-1,1)), single_significance.reshape((-1,1)))\n",
    "ax1.set_xlabel('Impact')\n",
    "ax1.set_ylabel('p-value')\n",
    "ax1.set_xticks(np.arange(-1.3, 0.9, 0.1))\n",
    "labels = ax1.get_xticklabels()\n",
    "plt.setp(labels, rotation=90)\n",
    "\n",
    "ax2 = fig.add_axes([0.675, 0.05, 0.3, 0.85])\n",
    "ax2.set_xlabel('value')\n",
    "ax2.set_ylabel('count')\n",
    "ax2.hist(single_impact.reshape((-1,1)), 25, facecolor='#ff7f0e', label='Impact')\n",
    "ax2.hist(single_significance.reshape((-1,1)), 25, facecolor='#2ca02c', alpha=0.75, label='p-value')\n",
    "ax2.legend(loc=0)\n",
    "\n",
    "plt.savefig(plot_dir + \"impact_vs_p-value.{}\".format(plot_format))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Especially in the additional histogram, it is clear that most p-values are quite small. Another good sign!\n",
    "\n",
    "Through trial and error we can probably figure the range of impact values corresponding to all the p-values over 0.05. This way we can easily say that all impact values below or above certain values are always significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T14:16:09.354259Z",
     "start_time": "2017-05-03T14:16:09.344420Z"
    }
   },
   "outputs": [],
   "source": [
    "imps = single_impact.reshape((-1,1))\n",
    "sigs = single_significance.reshape((-1,1))\n",
    "\n",
    "# The following values are the result of trial and error. You're welcome!\n",
    "neg_imps_A = imps < -.143\n",
    "neg_imps_B = imps < -.144\n",
    "\n",
    "pos_imps_A = imps > .228\n",
    "pos_imps_B = imps > .229\n",
    "\n",
    "print(\"Max p-value:            p = {} from {} values\".format(max(sigs)[0], len(sigs)))\n",
    "print()\n",
    "\n",
    "print(\"Max p-value I < -0.143: p = {} from {} values\".format(max(sigs[neg_imps_A]), len(sigs[neg_imps_A])))\n",
    "print(\"Max p-value I < -0.144: p = {} from {} values\".format(max(sigs[neg_imps_B]), len(sigs[neg_imps_B])))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Max p-value I >  0.228: p = {} from {} values\".format(max(sigs[pos_imps_A]), len(sigs[pos_imps_A])))\n",
    "print(\"Max p-value I >  0.229: p = {} from {} values\".format(max(sigs[pos_imps_B]), len(sigs[pos_imps_B])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the 324+118 impact values below -0.144 and above 0.229 are all guaranteed to be statistically significant for $p < 10^{-4}$, so that's good news too.\n",
    "\n",
    "Now we know that also the individual impact values make some sense, the easiest way to visualize them is probably with a heatmap, so we'll make one for each of the modules. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T14:17:10.974449Z",
     "start_time": "2017-05-03T14:16:09.355244Z"
    }
   },
   "outputs": [],
   "source": [
    "x = np.arange(len(fids)+1)\n",
    "y = np.arange(len(dims)+1)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "minval = np.min(single_impact)\n",
    "maxval = np.max(single_impact)\n",
    "\n",
    "y_tick_locs, y_tick_labels = zip(*[(i+.5, str(dim))] for i, dim in enumerate(dims))\n",
    "x_tick_locs, x_tick_labels = zip(*[(i+.5, str(fid))] for i, fid in enumerate(fids))\n",
    "\n",
    "for mod_i, mod in enumerate(modules):\n",
    "    plt.figure(figsize=(6,3))\n",
    "    plt.imshow(single_impact[:,:,mod_i], cmap='viridis', vmin=minval, vmax=maxval, origin='lower')\n",
    "    plt.title(\"Impact of {} module\".format(mod))\n",
    "    plt.colorbar(label='Impact')\n",
    "    plt.xlim([0,24])\n",
    "    plt.ylabel(\"Dimension\")\n",
    "    plt.yticks(y_tick_locs, y_tick_labels)\n",
    "    plt.xticks(x_tick_locs, x_tick_labels)\n",
    "    plt.xlabel(\"Function\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(plot_dir + \"impact_heatmap_{}.{}\".format(mod, plot_format))\n",
    "    plt.close()\n",
    "\n",
    "Image(filename='plots/impact_heatmap_Elitism.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The impact calculations we did above can be done for any two sets of configurations. So to determine the impact of a *combination* of modules, all we have to do is determine two other sets of configurations. For the combination set, this means the *on* set of configurations in which both modules are active, and the *off* set of everything else (think logical `AND` and `NAND` on the activation indices)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T14:17:39.357479Z",
     "start_time": "2017-05-03T14:17:10.975741Z"
    }
   },
   "outputs": [],
   "source": [
    "interaction_impact = np.zeros((len(dims), len(fids), len(modules), len(modules)))\n",
    "interaction_significance = np.zeros((len(dims), len(fids), len(modules), len(modules)))\n",
    "\n",
    "for i_dim, dim in enumerate(dims):\n",
    "    for i_fid, fid in enumerate(fids):\n",
    "        temp = cases[dim][fid]\n",
    "        \n",
    "        for A_mod in range(len(modules)):\n",
    "            A_indices_on = temp[:,A_mod] > 0\n",
    "            \n",
    "            for B_mod in range(len(modules)):\n",
    "                B_indices_on = temp[:,B_mod] > 0\n",
    "\n",
    "                indices_on = np.logical_and(A_indices_on, B_indices_on)  # A AND B\n",
    "                indices_off = np.logical_not(indices_on)                 # A NAND B == NOT (A AND B)\n",
    "\n",
    "                mean_on = np.mean(temp[indices_on,11])\n",
    "                mean_off = np.mean(temp[indices_off,11])\n",
    "\n",
    "                interaction_impact[i_dim, i_fid, A_mod, B_mod] = mean_off - mean_on\n",
    "                interaction_significance[i_dim, i_fid, A_mod, B_mod] = mannwhitneyu(temp[indices_on,11],\n",
    "                                                                                    temp[indices_off,11],\n",
    "                                                                                    alternative='two-sided')[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the single module analysis earlier, let's see what the spread of impact vs. p-value looks like to give an idea of how reliable the results are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T14:17:48.429602Z",
     "start_time": "2017-05-03T14:17:39.358503Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Interaction impact vs p-value\n",
    "\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "fig.suptitle('Interaction impact score and corresponding significance test values')\n",
    "\n",
    "ax1 = fig.add_axes([0.05, 0.05, 0.55, 0.85])\n",
    "ax1.set_title('Impact vs p-value')\n",
    "ax1.scatter(interaction_impact.reshape((-1,1)), interaction_significance.reshape((-1,1)), label='\\'nand\\' Impact')\n",
    "ax1.set_xlabel('Impact')\n",
    "ax1.set_ylabel('p-value')\n",
    "ax1.set_xticks(np.arange(-1.3, 0.9, 0.1))\n",
    "labels = ax1.get_xticklabels()\n",
    "plt.setp(labels, rotation=90)\n",
    "\n",
    "ax2 = fig.add_axes([0.675, 0.05, 0.3, 0.85])\n",
    "ax2.set_xlabel('value')\n",
    "ax2.set_ylabel('count')\n",
    "ax2.hist(interaction_impact.reshape((-1,1)), 25, facecolor='#ff7f0e', label='Impact')\n",
    "ax2.hist(interaction_significance.reshape((-1,1)), 25, facecolor='#2ca02c', alpha=0.75, label='p-value')\n",
    "ax2.legend(loc=0)\n",
    "\n",
    "plt.savefig(plot_dir + \"interaction_impact_vs_p-value.{}\".format(plot_format))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T14:17:48.455776Z",
     "start_time": "2017-05-03T14:17:48.432174Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Some statistics about the occurring p-values\n",
    "\n",
    "interaction_imps = interaction_impact.reshape((-1,1))\n",
    "interaction_sigs = interaction_significance.reshape((-1,1))\n",
    "\n",
    "sig_level = 0.01\n",
    "interaction_sig = interaction_sigs < sig_level\n",
    "\n",
    "# The following values are the result of trial and error. You're welcome!\n",
    "neg_imps_A = interaction_imps < -.552\n",
    "neg_imps_B = interaction_imps < -.553\n",
    "\n",
    "pos_imps_A = interaction_imps > .370\n",
    "pos_imps_B = interaction_imps > .371\n",
    "\n",
    "print(\"Max p-value:            p = {} from {} values\".format(max(interaction_sigs)[0], len(interaction_sigs)))\n",
    "print()\n",
    "\n",
    "print(\"Max p-value I < -0.552: p = {} from {} values\".format(max(interaction_sigs[neg_imps_A]), len(interaction_sigs[neg_imps_A])))\n",
    "print(\"Max p-value I < -0.553: p = {} from {} values\".format(max(interaction_sigs[neg_imps_B]), len(interaction_sigs[neg_imps_B])))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Max p-value I >  0.370: p = {} from {} values\".format(max(interaction_sigs[pos_imps_A]), len(interaction_sigs[pos_imps_A])))\n",
    "print(\"Max p-value I >  0.371: p = {} from {} values\".format(max(interaction_sigs[pos_imps_B]), len(interaction_sigs[pos_imps_B])))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Number of significant nand values (p < {}): {}/{} values\".format(sig_level, len(interaction_sigs[interaction_sig]), len(interaction_sigs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As could be expected, this is similar to the results in the single module case. At least this means we can generally be confident in drawing conclusions from the heatmaps we are about to create.\n",
    "\n",
    "In these cases, the interaction between the modules is most important, so we create 11\\*11 heatmaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T14:17:48.479533Z",
     "start_time": "2017-05-03T14:17:48.456639Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_and_save_interaction_impact(impacts, vmin=None, vmax=None, save=True, show=False, cmap='viridis',\n",
    "                                       extra_save_info='', title=None):\n",
    "\n",
    "    x = np.arange(len(modules)+1)\n",
    "    y = np.arange(len(modules)+1)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "\n",
    "    plt.figure(figsize=(5,4))\n",
    "    \n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    \n",
    "    plt.imshow(impacts, cmap=cmap, vmin=vmin, vmax=vmax, origin='lower')\n",
    "    plt.colorbar(label='Module Interaction Impact')\n",
    "    plt.xlim([0,len(modules)])\n",
    "    plt.ylim([0,len(modules)])\n",
    "    plt.xticks(np.arange(len(modules)) + 0.5, modules, rotation=45, horizontalalignment='right')\n",
    "    plt.yticks(np.arange(len(modules)) + 0.5, modules)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save:\n",
    "        plt.savefig(plot_dir + \"interaction_impact_heatmap{}.{}\".format(extra_save_info, plot_format))\n",
    "    \n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First up: impacts averaged over all 120 experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T14:17:51.875290Z",
     "start_time": "2017-05-03T14:17:48.482086Z"
    }
   },
   "outputs": [],
   "source": [
    "create_and_save_interaction_impact(np.mean(np.mean(interaction_impact, axis=0), axis=0), save=True, show=True, title=\"Impact of module interaction over all experiments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next: the same heatmaps, but separated for each function. This means that the values are still averaged over all tested dimensionalities. When comparing different functions, it is useful to have the colorbar in the same scale. However, when differences for a single function are small, it's better to just let the colorbar represent all impact values in just that plot. To be on the safe side, we plot them both. All plots using the same values are plotted using the `viridis` colormap, the others using the `plasma` colormap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T14:19:13.066503Z",
     "start_time": "2017-05-03T14:17:51.876840Z"
    }
   },
   "outputs": [],
   "source": [
    "minval = np.min(np.mean(interaction_impact, axis=0))\n",
    "maxval = np.max(np.mean(interaction_impact, axis=0))\n",
    "\n",
    "for fid in range(len(fids)):\n",
    "    plot_data = np.mean(interaction_impact[:,fid,:,:], axis=0)\n",
    "    \n",
    "    create_and_save_interaction_impact(plot_data, vmin=minval, vmax=maxval,\n",
    "                                       extra_save_info='_f{}'.format(fids[fid]),\n",
    "                                       title=\"Impact of module interaction for F{}\".format(fids[fid]))\n",
    "\n",
    "    create_and_save_interaction_impact(plot_data, cmap='plasma',\n",
    "                                       extra_save_info='_f{}_plasma'.format(fids[fid]),\n",
    "                                       title=\"Impact of module interaction for F{}\".format(fids[fid]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we repeat the process for each separate experiment. Again with the two different versions of the same plot, based on the scaling of the colormap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T14:28:13.621208Z",
     "start_time": "2017-05-03T14:19:13.067747Z"
    }
   },
   "outputs": [],
   "source": [
    "minval = np.min(interaction_impact)\n",
    "maxval = np.max(interaction_impact)\n",
    "\n",
    "for dim in range(len(dims)):\n",
    "    for fid in range(len(fids)):\n",
    "        create_and_save_interaction_impact(interaction_impact[dim,fid,:,:], vmin=minval, vmax=maxval,\n",
    "                                           extra_save_info='_{}_f{}'.format(dims[dim], fids[fid]),\n",
    "                                           title=\"Impact of module interaction for {}D F{}\".format(dims[dim], fids[fid]))\n",
    "\n",
    "        create_and_save_interaction_impact(interaction_impact[dim,fid,:,:], cmap='plasma',\n",
    "                                           extra_save_info='_{}_f{}_plasma'.format(dims[dim], fids[fid]),\n",
    "                                           title=\"Impact of module interaction for {}D F{}\".format(dims[dim], fids[fid]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! This is the analysis we performed, and should be exactly reproducible. If there are any problems, suggestions or remarks, feel free to mail me at: ` s.j.van.rijn@liacs.leidenuniv.nl `."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "389px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "844px",
    "left": "0px",
    "right": "1643px",
    "top": "107px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
